# 유투브 웹뷰 실습+ Arkit 공부

***

* 코드 참고하며 복붙 절대 안하고, 손으로 타이핑함 [유투브 웹뷰 실습](https://github.com/iOS-SOPT-iNNovation/iJoom/tree/master/YotubeWebView)
* 백문이 불어 일타 !
* Arkit공부 + 다음주에 더 보충 하겠심

***
* 애플 공식문서
### ARKit
Integrate iOS device camera and motion features to produce augmented reality experiences in your app or game.
***

### Overview
Augmented reality (AR) describes user experiences that add 2D or 3D elements to the live view from a device's camera in a way that makes those elements appear to inhabit the real world. ARKit combines device motion tracking, camera scene capture, advanced scene processing, and display conveniences to simplify the task of building an AR experience. You can create many kinds of AR experiences with these technologies using the front or rear camera of an iOS device.

***


iOS 기기 카메라 및 모션 기능을 통합하여 앱 또는 게임에서 증강 현실 경험을 제공합니다.

### iOS 11.0 이상

증강 현실 (AR)은 2D 또는 3D 요소를 장치의 카메라에서 라이브 뷰에 추가하여 해당 요소가 실제 세계에 거주하는 것처럼 보이는 사용자 경험을 설명합니다. ARKit은 장치 동작 추적, 카메라 장면 캡처, 고급 장면 처리 및 디스플레이 편의성을 결합하여 AR 경험 구축 작업을 단순화합니다. iOS 기기의 전면 또는 후면 카메라를 사용하여 이러한 기술로 다양한 AR 경험을 만들 수 있습니다.

***

### Augmented Reality with the Back Camera

The most common kinds of AR experience display a view from the device's back camera, augmented by other visual content, giving the user a new way to see and interact with the world around them.

[`ARWorldTrackingConfiguration`](https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration) provides this kind of experience: ARKit tracks the real-world the user inhabits, and matches it with a coordinate space for you to place virtual content. World tracking also offers features to make AR experiences more immersive, like recognizing objects and images in the user's environment and responding to real-world lighting conditions.

가장 일반적인 종류의 AR 경험은 장치의 후면 카메라에서 다른 시각적 컨텐츠로 보강 된 뷰를 표시하여 사용자가 주변 세계를보고 상호 작용할 수있는 새로운 방법을 제공합니다.
ARWorldTrackingConfiguration은 이러한 종류의 경험을 제공합니다. ARKit은 사용자가 거주하는 실제 세계를 추적하고 가상 콘텐츠를 배치 할 수있는 좌표 공간과 일치시킵니다. 세계 추적은 또한 사용자 환경에서 물체와 이미지를 인식하고 실제 조명 조건에 반응하는 것과 같이 AR 경험을보다 몰입 할 수있는 기능을 제공합니다.

### Augmented Reality with the Front Camera

For iOS devices that have a TrueDepth camera, [`ARFaceTrackingConfiguration`](https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration) enables you to augment the front-camera feed, while providing you with real-time tracking for the pose and expression of faces. With that information, that you might, for example, choose to overlay realistic virtual masks. Or, you might omit the camera view and use facial expression data to animate virtual characters, as done by the Animoji app for iMessage.



TrueDepth 카메라가있는 iOS 장치의 경우 ARFaceTrackingConfiguration을 사용하면 전면 카메라 피드를 확대하면서 얼굴의 포즈와 표정을 실시간으로 추적 할 수 있습니다. 예를 들어, 해당 정보를 사용하여 현실적인 가상 마스크를 오버레이하도록 선택할 수 있습니다. 또는 iMessage 용 Animoji 앱에서와 같이 카메라보기를 생략하고 표정 데이터를 사용하여 가상 캐릭터에 애니메이션을 적용 할 수 있습니다.

***

* 후면 카메라만 Ar이 있는 것이 아닌, 전면 카메라로 emoji를 만드는 것 또한 증강현실 + Ar sdk !!

***

### Basic Lifecycle of an AR Session

The figure below shows the changes in tracking state when you start running an AR session.

아래 그림은 AR 세션 실행을 시작할 때 추적 상태의 변경 사항을 보여줍니다.
사용 불가능에서 제한 (초기화), 정상으로 진행되는 ARKit 추적 상태의 시퀀스 다이어그램.
새 세션을 실행 한 직후 제공된 프레임의 추적 상태는 ARCamera.TrackingState.notAvailable이며, ARKit이 아직 장치의 포즈를 추정하기에 충분한 정보를 수집하지 않았음을 나타냅니다.

![Sequence diagram with ARKit tracking state proceeding from notAvailable to limited (initializing) to normal.](https://docs-assets.developer.apple.com/published/4151048f62/411c7573-20bf-418f-8982-8c8e6d48bfd5.png)



### Provide Feedback for Tracking Quality Changes

The figure below shows changes in tracking state that can occur due to user interaction or changes in the environment.아래 그림은 사용자 상호 작용 또는 환경의 변화로 인해 발생할 수있는 추적 상태의 변화를 보여줍니다.
ARKit 추적 상태가 정상에서 제한 (부족한 기능)으로 진행하고 다시 정상으로 진행되는 시퀀스 다이어그램.

![Sequence diagram with ARKit tracking state proceeding from normal to limited (insufficient features) and back to normal.](https://docs-assets.developer.apple.com/published/b563fd7340/bd7ab850-bf91-4ddf-b35b-cb0862c55d70.png)

***

```swift
@objc func didTap(_ gesture: UIPanGestureRecognizer) {
        // 1
        let tapLocation = gesture.location(in: self.sceneView)
        let results = self.sceneView.hitTest(tapLocation, types: .featurePoint)
 
        // 2
        guard let result = results.first else {
            return
        }
 
        // 3
        let translation = result.worldTransform.translation
 
        //4
        guard let node = self.node else {
            self.addBox(x: translation.x, y: translation.y, z: translation.z)
            return
        }
        node.position = SCNVector3Make(translation.x, translation.y, translation.z)
        self.sceneView.scene.rootNode.addChildNode(self.node)
    }
 
//5
extension float4x4 {
    var translation: float3 {
        let translation = self.columns.3
        return float3(translation.x, translation.y, translation.z)
    }
}
```

![img](https://miro.medium.com/max/718/0*6m71qS0gO41E6pdb.png)

* ARkit SceneKit View로 4*4 정사각형의 물체를 3D로 표현해 카메라로 직접 표현 시킴
* 화면에서 클릭하는 위치를 정의하고 해당 기능을 장면의 hitTest에 전달하면 표면과의 접촉 위치를 정의하는 대규모 ARHitTestResult가 나타남



***

예제)https://medium.com/@stfalconcom/augmented-reality-with-swift-5-how-to-start-19118c77dffe 

더 공부해오겠심 다음주 스터디떄